#!/usr/bin/env bash
# Copyright 2018 Barbarians.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
#
#Usage: start-hiveserver2 [OPTIONS]
# Starts a Hiveserver2 based on the supplied options.
#     --zk_connect        CNAME for the zookeeper ensemble

#     --rm_connect        CNAME address of the YARN Resource Manger

#     --igfs_connect      CNAME address of the IGFS cluster

#     --db-uri            Connect URI for the MySQL instance

#     --db-password       Passowrd for the Hive Meteastore instance

#     --db-username       Username for the hive metastore instance

#     --conf_dir          The directoyr where the Node Manager process will store its
#                         configuration. The default is /hadoop/etc/hadoop.

#     --webui_port        The port on which the Node Manager process will listen for 
#                         HTTP requests. The default is 8042.

#     --server_port       The port on which the Node Manager process will listen for 
#                         requests from other servers. The 
#                         default is 45454. 

#     --log_level         The log level for the zookeeeper server. Either FATAL,
#                         ERROR, WARN, INFO, DEBUG. The default is INFO.


USER=`whoami`
HOST=`hostname -s`
DOMAIN=`hostname -d`
LOG_LEVEL=INFO
CONF_DIR="/hadoop/etc/hadoop"
WEBUI_PORT=8042
SERVER_PORT=45454

function print_usage() {
echo "\
Usage: start-hiveserver2 [OPTIONS]
Starts a Hiveserver2 based on the supplied options.
     --zk_connect        CNAME for the zookeeper ensemble

     --rm_connect        CNAME address of the YARN Resource Manger

     --igfs_connect      CNAME address of the IGFS cluster

     --db_uri		 Connect URI for the MySQL instance

     --db_password       Passowrd for the Hive Meteastore instance

     --db_username       Username for the hive metastore instance

     --conf_dir          The directoyr where the Node Manager process will store its
                         configuration. The default is /hadoop/etc/hadoop.

     --webui_port        The port on which the Node Manager process will listen for 
                         HTTP requests. The default is 8042.

     --server_port       The port on which the Node Manager process will listen for 
                         requests from other servers. The 
                         default is 45454. 

     --log_level         The log level for the zookeeeper server. Either FATAL,
                         ERROR, WARN, INFO, DEBUG. The default is INFO.
"
}

function create_beeline_log4j() {
  rm -f $HIVE_CONF_DIR/$BEELINE_LOG4J
  cp /templates/hive/$BEELINE_LOG4J.template $HIVE_CONF_DIR/$BEELINE_LOG4J
}

function create_mapred_site_xml() {
  rm -f $HADOOP_CONF_DIR/$MAPRED_SITE_XML
  cp /templates/hadoop/$MAPRED_SITE_XML.template $HADOOP_CONF_DIR/$MAPRED_SITE_XML
}

function create_config_hive_site() {
  rm -f $HIVE_CONF_DIR/$CONFIG_FILE_HIVE_SITE
  TEMPLATE=`cat /templates/hive/$CONFIG_FILE_HIVE_SITE.template`
  TEMPLATE1="${TEMPLATE/zzzDB_URIzzz/$DB_URI}"
  TEMPLATE2="${TEMPLATE1/zzzDB_USERNAMEzzz/$DB_USERNAME}"
  TEMPLATE3="${TEMPLATE2/zzzDB_PASSWORDzzz/$DB_PASSWORD}"
  TEMPLATE4="${TEMPLATE3/zzzZK_CONNECTzzz/$ZK_CONNECT}"
  echo "$TEMPLATE4" > $CONF_DIR/$CONFIG_FILE_HIVE_SITE
}

function create_config_tez_site() {
  rm -f $TEZ_CONF_DIR/$CONFIG_FILE_TEZ_SITE
  TEMPLATE=`cat /templates/tez/$CONFIG_FILE_TEZ_SITE.template`
  echo "$TEMPLATE" > $TEZ_CONF_DIR/$CONFIG_FILE_TEZ_SITE
}

function create_config_core_site() {
    rm -f $CONFIG_FILE_CORE_SITE
    echo "\
<?xml version=\"1.0\" encoding=\"UTF-8\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<!--
    This file was autogenerated - do not modify!
-->
<configuration>
    <property>
        <name>hadoop.proxyuser.${USER}.groups</name>
        <value>*</value>
    </property>

    <property>
        <name>hadoop.proxyuser.${USER}.hosts</name>
        <value>*</value>
    </property>

    <property>
        <name>fs.igfs.impl</name>
        <value>org.apache.ignite.hadoop.fs.v1.IgniteHadoopFileSystem</value>
    </property>

    <property>
        <name>fs.AbstractFileSystem.igfs.impl</name>
        <value>org.apache.ignite.hadoop.fs.v2.IgniteHadoopFileSystem</value>
    </property>

    <property>
        <name>fs.defaultFS</name>
        <value>igfs://igfs@${IGFS_CONNECT}/</value>
    </property>

    <property>
        <name>fs.default.name</name>
        <value>igfs://igfs@${IGFS_CONNECT}/</value>
    </property>

    <property>
        <name>fs.igfs.name</name>
        <value>igfs://igfs@${IGFS_CONNECT}/</value>
    </property>
</configuration>
" >> $CONFIG_FILE_CORE_SITE
    cat $CONFIG_FILE_CORE_SITE >&2
}

function create_config_yarn_site() {
    rm -f $CONFIG_FILE_YARN_SITE
    echo "\
<?xml version=\"1.0\"?>
<configuration>
    <property>
      <name>yarn.acl.enable</name>
      <value>true</value>
    </property> 

    <property>
      <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
      <value>0.2</value>
    </property> 

    <property>
      <name>yarn.admin.acl</name>
      <value/>
    </property> 

    <property>
      <name>yarn.application.classpath</name>
      <value>${CONF_DIR}</value>
    </property>

    <property>
      <name>yarn.client.nodemanager-connect.max-wait-ms</name>
      <value>900000</value>
    </property>
    
    <property>
      <name>yarn.client.nodemanager-connect.retry-interval-ms</name>
      <value>10000</value>
    </property>

    <property>
      <name>yarn.log-aggregation-enable</name>
      <value>false</value>
    </property> 

    <property>
      <name>yarn.log-aggregation.retain-seconds</name>
      <value>2592000</value>
    </property> 

    <property>
      <name>yarn.log.server.url</name>
      <value></value>
    </property> 

    <property>
      <name>yarn.node-labels.fs-store.retry-policy-spec</name>
      <value>2000, 500</value>
    </property>
    
    <!-- Directory on HDFS used to store YARN node labels -->
    <property>
      <name>yarn.node-labels.fs-store.root-dir</name>
      <value>/system/yarn/node-labels</value>
    </property>
    
    <!-- To enable Node Labels, set the value to org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager -->
    <property>
      <name>yarn.node-labels.manager-class</name>
      <value>org.apache.hadoop.yarn.server.resourcemanager.nodelabels.MemoryRMNodeLabelsManager</value>
    </property>

    <property>
      <name>yarn.nodemanager.address</name>
      <value>0.0.0.0:${SERVER_PORT}</value>
    </property> 

    <property>
      <name>yarn.nodemanager.admin-env</name>
      <value>MALLOC_ARENA_MAX=\$MALLOC_ARENA_MAX</value>
    </property> 

    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property> 

    <property>
      <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>  

    <property>
      <name>yarn.nodemanager.bind-host</name>
      <value>0.0.0.0</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.container-monitor.interval-ms</name>
      <value>3000</value>
    </property> 

    <property>
      <name>yarn.nodemanager.delete.debug-delay-sec</name>
      <value>30000000</value>
    </property> 

    <property>
      <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
      <value>90</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb</name>
      <value>1000</value>
    </property>

    <property>
      <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
      <value>0.25</value>
    </property> 

    <property>
      <name>yarn.nodemanager.health-checker.interval-ms</name>
      <value>135000</value>
    </property> 

    <property>
      <name>yarn.nodemanager.health-checker.script.timeout-ms</name>
      <value>60000</value>
    </property>

    <!-- CGroup Isolation configurations -->
    <property>
      <name>yarn.nodemanager.linux-container-executor.cgroups.hierarchy</name>
      <value>hadoop-yarn</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.linux-container-executor.cgroups.mount</name>
      <value>false</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</name>
      <value>false</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.linux-container-executor.group</name>
      <value>hadoop</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.linux-container-executor.resources-handler.class</name>
      <value>org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler</value>
    </property>
    <!-- End: CGroup Isolation configurations -->  
    
    <property>
      <name>yarn.nodemanager.local-dirs</name>
      <value>/grid/0/yarn/local</value>
    </property> 

    <property>
      <name>yarn.nodemanager.log-aggregation.compression-type</name>
      <value>gz</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.log-aggregation.debug-enabled</name>
      <value>false</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.log-aggregation.num-log-files-per-app</name>
      <value>30</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds</name>
      <value>-1</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.log-dirs</name>
      <value>/grid/0/yarn/log</value>
    </property> 

    <property>
      <name>yarn.nodemanager.log.retain-second</name>
      <value>604800</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.recovery.dir</name>
      <value>/var/log/hadoop-yarn/nodemanager/recovery-state</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.recovery.enabled</name>
      <value>false</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.remote-app-log-dir</name>
      <value>/app-logs</value>
    </property> 

    <property>
      <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
      <value>logs</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.resource.cpu-vcores</name>
      <value>8</value>
    </property>
    
    <property>
      <name>yarn.nodemanager.resource.memory-mb</name>
      <value>8096</value>
    </property> 

    <property>
      <name>yarn.nodemanager.resource.percentage-physical-cpu-limit</name>
      <value>100</value>
    </property>

    <property>
      <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value>
    </property> 

    <property>
      <name>yarn.nodemanager.vmem-pmem-ratio</name>
      <value>2.1</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.address</name>
      <value>${RM_CONNECT}:8050</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.admin.address</name>
      <value>${RM_CONNECT}:8141</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.am.max-attempts</name>
      <value>2</value>
    </property>
        
    <property>
      <name>yarn.resourcemanager.connect.max-wait.ms</name>
      <value>900000</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.connect.retry-interval.ms</name>
      <value>30000</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.fs.state-store.retry-policy-spec</name>
      <value>2000, 500</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.fs.state-store.uri</name>
      <value> </value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>${RM_CONNECT}</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.nodes.exclude-path</name>
      <value>/etc/hadoop/conf/yarn.exclude</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.recovery.enabled</name>
      <value>false</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.resource-tracker.address</name>
      <value>${RM_CONNECT}:8025</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.scheduler.address</name>
      <value>${RM_CONNECT}:8030</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.scheduler.class</name>
      <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.state-store.max-completed-applications</name>
      <value>\${yarn.resourcemanager.max-completed-applications}</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.store.class</name>
      <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size</name>
      <value>10</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>
      <value>true</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.webapp.address</name>
      <value>${RM_CONNECT}:8088</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled</name>
      <value>false</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.work-preserving-recovery.enabled</name>
      <value>false</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms</name>
      <value>10000</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.zk-acl</name>
      <value>world:anyone:rwcda</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.zk-address</name>
      <value>${ZK_CONNECT}:2181</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.zk-num-retries</name>
      <value>1000</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.zk-retry-interval-ms</name>
      <value>1000</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.zk-state-store.parent-path</name>
      <value>/rmstore</value>
    </property>
    
    <property>
      <name>yarn.resourcemanager.zk-timeout-ms</name>
      <value>10000</value>
    </property>

    <property>
      <name>yarn.scheduler.maximum-allocation-mb</name>
      <value>4096</value>
    </property> 

    <property>
      <name>yarn.scheduler.minimum-allocation-mb</name>
      <value>682</value>
    </property> 

        <!-- 
            This sets the container framework that will launch and manage YARN
            containers.
            
            For Secure deployments, this should be set to:
            org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor
            -->
        <property>
      <name>yarn.nodemanager.container-executor.class</name>
      <value>org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor</value>
    </property>

        <!--
                This enables or disables wire encryption for YARN. 

                The possible values for this are:
                * HTTP_ONLY
                * HTTPS_ONLY
        -->
    <property>
      <name>yarn.http.policy</name>
      <value>HTTP_ONLY</value>
    </property> 

    <!-- ResourceManager HA settings

    <property> 
      <name>yarn.resourcemanager.ha.enabled</name> 
      <value>false</value>
    </property>

    <property> 
    <name>yarn.resourcemanager.ha.rm-ids</name> 
    <value>rm1,rm2</value> 
   </property>
   
   <property> 
      <name>yarn.resourcemanager.hostname.rm1</name> 
      <value>TODO-RESOURCEMANAGERNODE1-HOSTNAME</value> 
   </property> 
   
   <property> 
      <name>yarn.resourcemanager.hostname.rm2</name> 
      <value>TODO-RESOURCEMANAGERNODE2-HOSTNAME</value> 
   </property> 
   
   <property> 
      <name>yarn.resourcemanager.webapp.address.rm1</name> 
      <value>TODO-RESOURCEMANAGERNODE1-HOSTNAME:8088</value> 
      <description>We can set rm1_web_address separately. If not, it will use 
      \${yarn.resourcemanager.hostname.rm1}:DEFAULT_RM_WEBAPP_PORT</description> 
   </property> 
   
   <property> 
      <name>yarn.resourcemanager.webapp.address.rm2</name> 
      <value>TODO-RESOURCEMANAGERNODE2-HOSTNAME:8088</value> 
   </property> 
   
   <property> 
      <name>yarn.resourcemanager.recovery.enabled</name> 
      <value>true</value> 
   </property> 
   
   <property> 
      <name>yarn.resourcemanager.store.class</name> 
      <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value> 
   </property> 
   
   <property> 
     <name>yarn.client.failover-proxy-provider</name> 
     <value>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</value> 
   </property>

    -->

    <!-- Secure cluster configurations section start

        - If LinuxContainerExecutor is being used, this configuration is
        - needed
    <property>
      <name>yarn.nodemanager.linux-container-executor.group</name>
      <value>hadoop</value>
    </property> 

    <property>
      <name>yarn.nodemanager.keytab</name>
      <value>/etc/security/keytabs/nm.service.keytab</value>
    </property> 

    <property>
      <name>yarn.nodemanager.principal</name>
      <value>nm/_HOST@TODO-KERBEROS-DOMAIN</value>
    </property> 

    <property>
      <name>yarn.nodemanager.webapp.spnego-keytab-file</name>
      <value>/etc/security/keytabs/spnego.service.keytab</value>
    </property> 

    <property>
      <name>yarn.nodemanager.webapp.spnego-principal</name>
      <value>HTTP/_HOST@TODO-KERBEROS-DOMAIN</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.keytab</name>
      <value>/etc/security/keytabs/rm.service.keytab</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.principal</name>
      <value>rm/_HOST@TODO-KERBEROS-DOMAIN</value>
    </property> 

        <property>
      <name>yarn.resourcemanager.webapp.spnego-keytab-file</name>
      <value>/etc/security/keytabs/spnego.service.keytab</value>
    </property> 

    <property>
      <name>yarn.resourcemanager.webapp.spnego-principal</name>
      <value>HTTP/_HOST@TODO-KERBEROS-DOMAIN</value>
    </property>

    <property>
      <name>mapreduce.jobhistory.webapp.spnego-principal</name>
      <value>HTTP/_HOST@EXAMPLE.COM</value>
    </property>

    <property>
      <name>mapreduce.jobhistory.webapp.spnego-keytab-file</name>
      <value>/etc/security/keytabs/spnego.service.keytab</value>
    </property>
        
        - Timeline Server HTTPs configuration
    <property>
      <name>yarn.timeline-service.webapp.https.address</name>
      <value>TODO-TIMELINESERVER-HOSTNAME:8190</value>
    </property>

     Secure cluster configurations section end -->
    
</configuration>
" >> $CONFIG_FILE_YARN_SITE
    cat $CONFIG_FILE_YARN_SITE >&2
}

function create_hadoop_env() {
    rm -f $HADOOP_ENV_FILE
    echo "\
export HADOOP_HOME=/hadoop
export YARN_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
" >> $HADOOP_ENV_FILE
    cat $HADOOP_ENV_FILE >&2
}

function create_hive_log() {
   rm -f $HIVE_LOG_FILE
   cp /templates/hive/$HIVE_LOG_FILE.template /hive/conf/$HIVE_LOG_FILE
}

function create_log_props() {
    rm -f $LOGGER_PROPS_FILE
    echo "Creating NM log4j configuration"
    echo "\
hadoop.root.logger=INFO,console
hadoop.log.dir=.
hadoop.log.file=hadoop.log

# Define the root logger to the system property \"hadoop.root.logger\".
log4j.rootLogger=\${hadoop.root.logger}, EventCounter

# Logging Threshold
log4j.threshold=ALL

# Null Appender
log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender

#
# Rolling File Appender - cap space usage at 5gb.
#
hadoop.log.maxfilesize=256MB
hadoop.log.maxbackupindex=20
log4j.appender.RFA=org.apache.log4j.RollingFileAppender
log4j.appender.RFA.File=\${hadoop.log.dir}/\${hadoop.log.file}

log4j.appender.RFA.MaxFileSize=\${hadoop.log.maxfilesize}
log4j.appender.RFA.MaxBackupIndex=\${hadoop.log.maxbackupindex}

log4j.appender.RFA.layout=org.apache.log4j.PatternLayout

# Pattern format: Date LogLevel LoggerName LogMessage
log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
# Debugging Pattern format
#log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n


#
# Daily Rolling File Appender
#

log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.DRFA.File=\${hadoop.log.dir}/\${hadoop.log.file}

# Rollover at midnight
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd

log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout

# Pattern format: Date LogLevel LoggerName LogMessage
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
# Debugging Pattern format
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n


log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n

#
# TaskLog Appender
#
log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender

log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n

#
# HDFS block state change log from block manager
#
# Uncomment the following to log normal block state change
# messages from BlockManager in NameNode.
#log4j.logger.BlockStateChange=DEBUG

#
#Security appender
#
hadoop.security.logger=INFO,NullAppender
hadoop.security.log.maxfilesize=256MB
hadoop.security.log.maxbackupindex=20
log4j.category.SecurityLogger=\${hadoop.security.logger}
hadoop.security.log.file=SecurityAuth-\${user.name}.audit
log4j.appender.RFAS=org.apache.log4j.RollingFileAppender
log4j.appender.RFAS.File=\${hadoop.log.dir}/\${hadoop.security.log.file}
log4j.appender.RFAS.layout=org.apache.log4j.PatternLayout
log4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
log4j.appender.RFAS.MaxFileSize=\${hadoop.security.log.maxfilesize}
log4j.appender.RFAS.MaxBackupIndex=\${hadoop.security.log.maxbackupindex}

#
# Daily Rolling Security appender
#
log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender
log4j.appender.DRFAS.File=\${hadoop.log.dir}/\${hadoop.security.log.file}
log4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout
log4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
log4j.appender.DRFAS.DatePattern=.yyyy-MM-dd

#
# hadoop configuration logging
#

# Uncomment the following line to turn off configuration deprecation warnings.
# log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN

#
# hdfs audit logging
#
hdfs.audit.logger=INFO,NullAppender
hdfs.audit.log.maxfilesize=256MB
hdfs.audit.log.maxbackupindex=20
log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=\${hdfs.audit.logger}
log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
log4j.appender.RFAAUDIT=org.apache.log4j.RollingFileAppender
log4j.appender.RFAAUDIT.File=\${hadoop.log.dir}/hdfs-audit.log
log4j.appender.RFAAUDIT.layout=org.apache.log4j.PatternLayout
log4j.appender.RFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
log4j.appender.RFAAUDIT.MaxFileSize=\${hdfs.audit.log.maxfilesize}
log4j.appender.RFAAUDIT.MaxBackupIndex=\${hdfs.audit.log.maxbackupindex}

#
# NameNode metrics logging.
# The default is to retain two namenode-metrics.log files up to 64MB each.
#
namenode.metrics.logger=INFO,NullAppender
log4j.logger.NameNodeMetricsLog=\${namenode.metrics.logger}
log4j.additivity.NameNodeMetricsLog=false
log4j.appender.NNMETRICSRFA=org.apache.log4j.RollingFileAppender
log4j.appender.NNMETRICSRFA.File=\${hadoop.log.dir}/namenode-metrics.log
log4j.appender.NNMETRICSRFA.layout=org.apache.log4j.PatternLayout
log4j.appender.NNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
log4j.appender.NNMETRICSRFA.MaxBackupIndex=1
log4j.appender.NNMETRICSRFA.MaxFileSize=64MB

#
# DataNode metrics logging.
# The default is to retain two datanode-metrics.log files up to 64MB each.
#
datanode.metrics.logger=INFO,NullAppender
log4j.logger.DataNodeMetricsLog=\${datanode.metrics.logger}
log4j.additivity.DataNodeMetricsLog=false
log4j.appender.DNMETRICSRFA=org.apache.log4j.RollingFileAppender
log4j.appender.DNMETRICSRFA.File=\${hadoop.log.dir}/datanode-metrics.log
log4j.appender.DNMETRICSRFA.layout=org.apache.log4j.PatternLayout
log4j.appender.DNMETRICSRFA.layout.ConversionPattern=%d{ISO8601} %m%n
log4j.appender.DNMETRICSRFA.MaxBackupIndex=1
log4j.appender.DNMETRICSRFA.MaxFileSize=64MB

# Custom Logging levels

#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG
#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG
#log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG


# AWS SDK & S3A FileSystem
#log4j.logger.com.amazonaws=ERROR
log4j.logger.com.amazonaws.http.AmazonHttpClient=ERROR
#log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN

#
# Event Counter Appender
# Sends counts of logging messages at different severity levels to Hadoop Metrics.
#
log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter

#
# Job Summary Appender
#
# Use following logger to send summary to separate file defined by
# hadoop.mapreduce.jobsummary.log.file :
# hadoop.mapreduce.jobsummary.logger=INFO,JSA
# 
hadoop.mapreduce.jobsummary.logger=\${hadoop.root.logger}
hadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log
hadoop.mapreduce.jobsummary.log.maxfilesize=256MB
hadoop.mapreduce.jobsummary.log.maxbackupindex=20
log4j.appender.JSA=org.apache.log4j.RollingFileAppender
log4j.appender.JSA.File=\${hadoop.log.dir}/\${hadoop.mapreduce.jobsummary.log.file}
log4j.appender.JSA.MaxFileSize=\${hadoop.mapreduce.jobsummary.log.maxfilesize}
log4j.appender.JSA.MaxBackupIndex=\${hadoop.mapreduce.jobsummary.log.maxbackupindex}
log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
log4j.appender.JSA.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
log4j.logger.org.apache.hadoop.mapred.JobInProgress\$JobSummary=\${hadoop.mapreduce.jobsummary.logger}
log4j.additivity.org.apache.hadoop.mapred.JobInProgress\$JobSummary=false

#
# shuffle connection log from shuffleHandler
# Uncomment the following line to enable logging of shuffle connections
# log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG

#
# Yarn ResourceManager Application Summary Log
#
# Set the ResourceManager summary log filename
yarn.server.resourcemanager.appsummary.log.file=rm-appsummary.log
# Set the ResourceManager summary log level and appender
yarn.server.resourcemanager.appsummary.logger=\${hadoop.root.logger}
#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY

# To enable AppSummaryLogging for the RM,
# set yarn.server.resourcemanager.appsummary.logger to
# <LEVEL>,RMSUMMARY in hadoop-env.sh

# Appender for ResourceManager Application Summary Log
# Requires the following properties to be set
#    - hadoop.log.dir (Hadoop Log directory)
#    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)
#    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)

log4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\$ApplicationSummary=\${yarn.server.resourcemanager.appsummary.logger}
log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\$ApplicationSummary=false
log4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender
log4j.appender.RMSUMMARY.File=\${hadoop.log.dir}/\${yarn.server.resourcemanager.appsummary.log.file}
log4j.appender.RMSUMMARY.MaxFileSize=256MB
log4j.appender.RMSUMMARY.MaxBackupIndex=20
log4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout
log4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n

# Appender for viewing information for errors and warnings
yarn.ewma.cleanupInterval=300
yarn.ewma.messageAgeLimitSeconds=86400
yarn.ewma.maxUniqueMessages=250
log4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender
log4j.appender.EWMA.cleanupInterval=\${yarn.ewma.cleanupInterval}
log4j.appender.EWMA.messageAgeLimitSeconds=\${yarn.ewma.messageAgeLimitSeconds}
log4j.appender.EWMA.maxUniqueMessages=\${yarn.ewma.maxUniqueMessages}

# Log levels of third-party libraries
log4j.logger.org.apache.commons.beanutils=WARN
" >> $LOGGER_PROPS_FILE
}

optspec=":hv-:"
while getopts "$optspec" optchar; do

    case "${optchar}" in
        -)
            case "${OPTARG}" in
                conf_dir=*)
                    CONF_DIR=${OPTARG##*=}
                    ;;
                webui_port=*)
                    WEBUI_PORT=${OPTARG##*=}
                    ;;
                server_port=*)
                    SERVER_PORT=${OPTARG##*=}
                    ;;
                zk_connect=*)
                    ZK_CONNECT=${OPTARG##*=}
                    ;;
                rm_connect=*)
                    RM_CONNECT=${OPTARG##*=}
                    ;;
                igfs_connect=*)
                    IGFS_CONNECT=${OPTARG##*=}
                    ;;
                log_level=*)
                    LOG_LEVEL=${OPTARG##*=}
                    ;;
                db_uri=*)
                    DB_URI=${OPTARG##*=}
                    ;;
                db_username=*)
                    DB_USERNAME=${OPTARG##*=}
                    ;;
                db_password=*)
                    DB_PASSWORD=${OPTARG##*=}
                    ;;
                *)
                    echo "Unknown option --${OPTARG}" >&2
                    exit 1
                    ;;
            esac;;
        h)
            print_usage
            exit
            ;;
        v)
            echo "Parsing option: '-${optchar}'" >&2
            ;;
        *)
            if [ "$OPTERR" != 1 ] || [ "${optspec:0:1}" = ":" ]; then
                echo "Non-option argument: '-${OPTARG}'" >&2
            fi
            ;;
    esac
done

CONFIG_FILE_TEZ_SITE="$tez-site.xml"
CONFIG_FILE_HIVE_SITE="hive-site.xml"
CONFIG_FILE_CORE_SITE="$HADOOP_CONF_DIR/core-site.xml"
CONFIG_FILE_YARN_SITE="$HADOOP_CONF_DIR/yarn-site.xml"
LOGGER_PROPS_FILE="$HADOOP_CONF_DIR/log4j.properties"
HADOOP_ENV_FILE="$HADOOP_CONF_DIR/hadoop-env.sh"
HIVE_LOG_FILE="hive-log4j2.properties"
BEELINE_LOG4J="beeline-log4j2.properties"
MAPRED_SITE_XML="mapred-site.xml"

export HADOOP_CLASSPATH=$HADOOP_CLASSPATH
export HIVE_CLASSPATH=$HADOOP_CLASSPATH

create_beeline_log4j && create_mapred_site_xml && \
create_log_props && create_config_tez_site && create_config_hive_site && create_config_core_site && create_config_yarn_site && create_hadoop_env && create_log_props && exec $HIVE_HOME/bin/hiveserver2
